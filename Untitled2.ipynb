{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMZlkTmMrtbUYHwgUTPS/NE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albercej/zrh/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch LSTMs for time-series data\n",
        "Using the Pytorch functional API to build temporal models for univariate time series\n",
        "\n",
        "Jan 12, 2022 • 31 min read\n",
        "\n",
        "lstm   pytorch\n",
        "\n",
        "Introduction\n",
        "Simplest possible neural network\n",
        "Intuition behind LSTMs\n",
        "Pytorch LSTM\n",
        "Generating the data\n",
        "Initialisation\n",
        "Forward method\n",
        "Training the model\n",
        "Automating model construction\n",
        "Conclusion"
      ],
      "metadata": {
        "id": "E49CfEgOzvlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introduction\n",
        "You might have noticed that, despite the frequency with which we encounter sequential data in the real world, there isn’t a huge amount of content online showing how to build simple LSTMs from the ground up using the Pytorch functional API. Even the LSTM example on Pytorch’s official documentation only applies it to a natural language problem, which can be disorienting when trying to get these recurrent models working on time series data. In this article, we’ll set a solid foundation for constructing an end-to-end LSTM, from tensor input and output shapes to the LSTM itself. \n",
        "\n",
        "This article is structured with the goal of being able to implement any univariate time-series LSTM. We begin by examining the shortcomings of traditional neural networks for these tasks, and why an LSTM’s input is differently shaped to simple neural nets. We’ll then intuitively describe the mechanics that allow an LSTM to “remember.” With this approximate understanding, we can implement a Pytorch LSTM using a traditional model class structure inheriting from nn.Module, and write a forward method for it. We use this to see if we can get the LSTM to learn a simple sine wave. Finally, we attempt to write code to generalise how we might initialise an LSTM based on the problem at hand, and test it on our previous examples.\n",
        "\n",
        "Simplest possible neural network\n",
        "Let’s suppose we have the following time-series data. Rather than using complicated recurrent models, we’re going to treat the time series as a simple input-output function: the input is the time, and the output is the value of whatever dependent variable we’re measuring. This is essentially just simplifying a univariate time series.\n",
        "\n",
        "You might be wondering there’s any difference between the problem we’ve outlined above, and an actual sequential modelling approach to time series problems (as used in LSTMs). The difference is in the recurrency of the solution. Here, we’re simply passing in the current time step and hoping the network can output the function value. However, in recurrent neural networks, we not only pass in the current input, but also previous outputs. In this way, the network can learn dependencies between previous function values and the current one. Here, the network has no way of learning these dependencies, because we simply don’t input previous outputs into the model.\n",
        "\n",
        "Let’s suppose that we’re trying to model the number of minutes Klay Thompson will play in his return from injury. Steve Kerr, the coach of the Golden State Warriors, doesn’t want Klay to come back and immediately play heavy minutes. Instead, he will start Klay with a few minutes per game, and ramp up the amount of time he’s allowed to play as the season goes on. We’re going to be Klay Thompson’s physio, and we need to predict how many minutes per game Klay will be playing in order to determine how much strapping to put on his knee. \n",
        "\n",
        "Thus, the number of games since returning from injury (representing the input time step) is the independent variable, and Klay Thompson’s number of minutes in the game is the dependent variable. Suppose we observe Klay for 11 games, recording his minutes per game in each outing to get the following data."
      ],
      "metadata": {
        "id": "3J51SbP21DZZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "bAYtaoY9xLYG"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = [x for x in range(11)]\n",
        "y = [1.6*x + 4 + np.random.normal(10, 1) for x in X]\n",
        "X, y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nDSX-J5xSdi",
        "outputId": "0bb8a9ab-782d-4f2a-f39b-7688b32cda76"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
              " [13.616763864431123,\n",
              "  17.190268308021665,\n",
              "  18.20141992268493,\n",
              "  17.531431402727215,\n",
              "  19.428141617774642,\n",
              "  21.33174116892158,\n",
              "  23.98749412075143,\n",
              "  25.132150528627186,\n",
              "  27.31935797828541,\n",
              "  28.018599768687828,\n",
              "  31.893517269803862])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we’ve generated the minutes per game as a linear relationship with the number of games since returning. We’re going to use 9 samples for our training set, and 2 samples for validation."
      ],
      "metadata": {
        "id": "i9ceX6Iy1gIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X[:9]\n",
        "y_train = y[:9]\n",
        "X_val = X[9:]\n",
        "y_val = X[9:]"
      ],
      "metadata": {
        "id": "ZqRLMi1WxTVL"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know that the relationship between game number and minutes is linear. However, we’re still going to use a non-linear activation function, because that’s the whole point of a neural network. (Otherwise, this would just turn into linear regression: the composition of linear operations is just a linear operation.) As per usual, we use nn.Sequential to build our model with one hidden layer, with 13 hidden neurons."
      ],
      "metadata": {
        "id": "C3C5eMJe1NLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "9i_5LYgy3haz"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_model = nn.Sequential(\n",
        "    nn.Linear(1, 13),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(13, 1))\n",
        "\n",
        "seq_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdRqWWlpxmCT",
        "outputId": "dd95c242-1cd8-4cbb-c3f8-994afad75422"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=1, out_features=13, bias=True)\n",
              "  (1): Tanh()\n",
              "  (2): Linear(in_features=13, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now need to write a training loop, as we always do when using gradient descent and backpropagation to force a network to learn. To remind you, each training step has several key tasks:\n",
        "\n",
        "Compute the forward pass through the network by applying the model to the training examples.\n",
        "Calculate the loss based on the defined loss function, which compares the model output to the actual training labels.\n",
        "Backpropagate the derivative of the loss with respect to the model parameters through the network. This is done with call .backward() on the loss, after setting the current parameter gradients to zero with .zero_grad().\n",
        "Update the model parameters by subtracting the gradient times the learning rate. This is done with our optimiser, using optimiser.step()."
      ],
      "metadata": {
        "id": "rIdi2kdL1WQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(n_epochs, optimiser, model, loss_fn, X_train,  X_val, y_train, y_val):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        output_train = model(X_train) # forwards pass\n",
        "        loss_train = loss_fn(output_train, y_train) # calculate loss\n",
        "        output_val = model(X_val) \n",
        "        loss_val = loss_fn(output_val, y_val)\n",
        "        \n",
        "        optimiser.zero_grad() # set gradients to zero\n",
        "        loss_train.backward() # backwards pass\n",
        "        optimiser.step() # update model parameters\n",
        "        if epoch == 1 or epoch % 10000 == 0:\n",
        "            print(f\"Epoch {epoch}, Training loss {loss_train.item():.4f},\"\n",
        "                  f\" Validation loss {loss_val.item():.4f}\")"
      ],
      "metadata": {
        "id": "dK9H7bhKxp0F"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimiser = optim.SGD(seq_model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "ukpfiYBK5cWW"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loop(\n",
        "    n_epochs = 500000, \n",
        "    optimiser = optimiser,\n",
        "    model = seq_model,\n",
        "    loss_fn = nn.MSELoss(),\n",
        "    X_train = X_train,\n",
        "    X_val = X_val, \n",
        "    y_train = y_train,\n",
        "    y_val = y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "Xbmi6zywx9W6",
        "outputId": "e9a72d54-965f-4789-b7e3-20e2d3ba72ea"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-6abadf14571f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     y_val = y_val)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-2442f12ec860>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(n_epochs, optimiser, model, loss_fn, X_train, X_val, y_train, y_val)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0moutput_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# forwards pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moutput_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not list"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "N = 100 # number of samples\n",
        "L = 1000 # length of each sample (number of values for each sine wave)\n",
        "T = 20 # width of the wave\n",
        "x = np.empty((N,L), np.float32) # instantiate empty array\n",
        "x[:] = np.arange(L)) + np.random.randint(-4*T, 4*T, N).reshape(N,1)\n",
        "y = np.sin(x/1.0/T).astype(np.float32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "qbqYnkdayB2Q",
        "outputId": "2aaa8287-f822-400f-f8dc-99acb2259b2b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-fdb6cb57be31>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    x[:] = np.arange(L)) + np.random.randint(-4*T, 4*T, N).reshape(N,1)\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, hidden_layers=64):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_layers = hidden_layers\n",
        "        # lstm1, lstm2, linear are all layers in the network\n",
        "        self.lstm1 = nn.LSTMCell(1, self.hidden_layers)\n",
        "        self.lstm2 = nn.LSTMCell(self.hidden_layers, self.hidden_layers)\n",
        "        self.linear = nn.Linear(self.hidden_layers, 1)\n",
        "        \n",
        "    def forward(self, y, future_preds=0):\n",
        "        outputs, num_samples = [], y.size(0)\n",
        "        h_t = torch.zeros(n_samples, self.hidden_layers, dtype=torch.float32)\n",
        "        c_t = torch.zeros(n_samples, self.hidden_layers, dtype=torch.float32)\n",
        "        h_t2 = torch.zeros(n_samples, self.hidden_layers, dtype=torch.float32)\n",
        "        c_t2 = torch.zeros(n_samples, self.hidden_layers, dtype=torch.float32)\n",
        "        \n",
        "        for time_step in y.split(1, dim=1):\n",
        "            # N, 1\n",
        "            h_t, c_t = self.lstm1(input_t, (h_t, c_t)) # initial hidden and cell states\n",
        "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2)) # new hidden and cell states\n",
        "            output = self.linear(h_t2) # output from the last FC layer\n",
        "            outputs.append(output)\n",
        "            \n",
        "        for i in range(future_preds):\n",
        "            # this only generates future predictions if we pass in future_preds>0\n",
        "            # mirrors the code above, using last output/prediction as input\n",
        "            h_t, c_t = self.lstm1(output, (h_t, c_t))\n",
        "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
        "            output = self.linear(h_t2)\n",
        "            outputs.append(output)\n",
        "        # transform list to tensor    \n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "        return outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "SGvkeoVHyIcR",
        "outputId": "d24af27e-1514-449d-f554-e778850edbcb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-745a74e3c434>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# lstm1, lstm2, linear are all layers in the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.from_numpy(y[3:, :-1])\n",
        "b = a.split(1, dim=1)\n",
        "b[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "5zbpbFa8yOhh",
        "outputId": "87fa1395-37af-46bc-8f4c-77395825c551"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-39544241b282>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pMzavtL4yS8V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}